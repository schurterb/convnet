# convolutional_network
######Python functions for creating, training and testing 3D convolutional neural networks.

   The convolutional neural networks generated by CNN() are purely convolutional,
no pooling, dropout, or regression. It can be of any depth or width with any 
size of convolutional filters, provided the user's machine has the memory for 
exceptionally large networks. The networks are intended to produce affinity graphs
indicating the probability that adjacent pixels are part of the same object. These
affinity graphs can then be connected via a connected-components algorith to 
produce a segmented image.

-------------------------------------------------------------------------------
###Defining Networks
All user-definable parameters for building, training, and testing a network are
defined in network.ini. Parameters defining the structure of the network are
listed under the [Network] section.

```
[Network]
activation = relu
cost_func = MSE
load_weights = True
weights_folder = test/weights/

num_layers = 5
num_filters = 6
filter_size = 5
```

Current options for activation functions include
    relu for Rectified Linear Units
    tanh for Hyperbolic Tangent
    sig for Sigmoid

Current options for cost functions include MSE and binary crossentropy.

If load_weights is True, then it does not matter what the last three parameters
are set to. CNN will determine the structure of the network based on the format
of the weight files.

-------------------------------------------------------------------------------
## Training Networks
Parameters defining the training regiment for the network are listed under the
[Training] section. By default, all training is done stochastically.

```
[Training]
trainer_folder = results/test/trainer/
learning_method = malis
learning_rate = 0.0001
beta1 = 0.9
beta2 = 0.9
damping = 1.0e-8
batch_size = 50
num_epochs = 1
early_stop = False
```

Current options for learning methods include
    standardSGD with a constant learning rate (default)
    RMSprop - beta2 is the decay factor
    ADAM - beta2 is the decay for the variance term (as with RMSprop)
	   beta1 is the decay for the momentum term
    malis - see this [paper](http://papers.nips.cc/paper/3887-maximin-affinity-learning-of-image-segmentation)

Epoch size is defined to be approximately the entire training set and is, thus, not configurable.

Setting the early_stop flag allows the trainer to automatically stop when the cost stops decreasing.
(Note, this does not work for malis.)

If a trainer folder (created by a previous training period) is available, 
the training can be restarted without any discontinuity. 


```
python train_network.py -c path/to/network.ini
```

Similarly, predictions can be made on the test set by calling

```
python test_network.py --c path/to/network.ini
```

-------------------------------------------------------------------------------
